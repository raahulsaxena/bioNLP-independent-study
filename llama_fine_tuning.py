# -*- coding: utf-8 -*-
"""llama-fine-tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T0IYCJwKMHWWlePDwbK7tPtNAjNLi21h
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Installs Unsloth, Xformers (Flash Attention) and all other packages!
# !pip install unsloth
# # Get latest Unsloth
# !pip install --upgrade --no-deps "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

"""## Initializing the Model"""

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-v0.3-bnb-4bit",      # New Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit",           # Llama-3 15 trillion tokens model 2x faster!
    "unsloth/llama-3-8b-Instruct-bnb-4bit",
    "unsloth/llama-3-70b-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct",        # Phi-3 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",             # Gemma 2.2x faster!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

"""# Dataset Preparation :
Loading the question_with_options along with the predicted_label and the gpt_reasoning information as a csv.
"""

## DataSet Preparation

from datasets import load_dataset
dataset = load_dataset(
    "csv",
    data_files = "dataset.csv",
    split = "train",
)
print(dataset.column_names)
print(dataset[0])

"""## Converting the dataset into required format:"""

from unsloth import to_sharegpt

# Define a post-processing function to combine 'predicted_label' and 'reasoning'
def combine_columns(example):
    example["predicted_label_and_reasoning"] = f"Label: {example['predicted_label']}, Reasoning: {example['gpt_reasoning']}"
    return example

# Apply the function to the dataset
dataset = dataset.map(combine_columns)

# Verify the formatted dataset
print(dataset[0])


# Merge "predicted_label" and "reasoning" into the response in the merged_prompt
dataset = to_sharegpt(
    dataset,
    merged_prompt="{questions_with_options}",
    output_column_name="predicted_label_and_reasoning",  # Combine predicted_label and reasoning
)


print(dataset[0])

"""## Standardize share_gpt"""

from unsloth import standardize_sharegpt
dataset = standardize_sharegpt(dataset)

"""## Chat Template"""

chat_template = """Below is a question with options. Provide the predicted label and reasoning.

>>> Question:
{INPUT}

>>> Answer:
{OUTPUT}"""

from unsloth import apply_chat_template
dataset = apply_chat_template(
    dataset,
    tokenizer = tokenizer,
    chat_template = chat_template,
    # default_system_message = "You are a helpful assistant", << [OPTIONAL]
)

"""## Training the model
<a name="Train"></a>
Defining the training parametes and using Huggingface TRL's `SFTTrainer`!
"""

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        # max_steps = 60,
        num_train_epochs = 5,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)

trainer_stats = trainer.train()

"""## Saving the fine-tune model"""

# Step 9: Save the fine-tuned model
model.save_pretrained("fine_tuned_llama")
tokenizer.save_pretrained("fine_tuned_llama")



"""## Performing Inference"""

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

# Define the input message
messages = [
    {"role": "user", "content": 'A 62-year-old man comes to the physician because of hematemesis and progressive heartburn over the past 5 days. '\
                                'Ten days ago, he was started on a medication to treat a condition that causes hearing difficulties and pain in the lower extremities. '\
                                'He has no other history of serious illness. He has smoked one pack of cigarettes daily for the past 20 years. '\
                                'Vital signs are within normal limits. Physical examination shows bowing of the tibias. '\
                                'Upper endoscopy shows inflammation of the mucosa and a 1-cm punched-out ulcer in the distal esophagus. '\
                                'Which of the following drugs is the most likely cause of the patient\'s current condition?\n\n'\
                                'These are the options:\n\n'\
                                'A. Calcium citrate\n'\
                                'B. Denosumab\n'\
                                'C. Risedronate\n'\
                                'D. Calcitonin\n'\
                                'E. Prednisolone\n'\
                                'F. Acetaminophen\n\n'\
                                'Label this question\'s difficulty as Easy, Medium, or Hard, and provide a reasoning for your label.'},
]

# Apply the chat template and convert to input IDs
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt",
).to("cuda")

# Set up a streamer for output generation
from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt=True)

# Generate the response
_ = model.generate(
    input_ids,
    streamer=text_streamer,
    max_new_tokens=256,
    pad_token_id=tokenizer.eos_token_id
)

# Step 10: Perform inference with the fine-tuned model
from transformers import pipeline

generator = pipeline("text2text-generation", model="fine_tuned_llama", tokenizer="fine_tuned_llama")
question = "What is 2 + 2? Choose from: 1, 2, 3, 4."
response = generator(question, max_length=100)
print(response)

"""## Evaluation Metrics and pre-defined function

Function name: evaluate_difficulty_and_reasoning
Function Path: project/pi_hongyu_umass_edu/zonghai/6_usmle_qg_quality_difficulty/anirudh/bleurt/evaluation_script_diff_reasonging.py
Usage Example:

```
###Testing
import pandas as pd

# Sample data
data = {
    "difficulty": [1, 2, 1, 3],  # Ground truth difficulty levels
    "difficulty_prediction": [1, 2, 1, 2],  # Predicted difficulty levels
    "reasoning": [
        "The questionnaire is simple and does not require detailed medical history.",
        "The questionnaire involves multiple questions about medical history.",
        "The questions are straightforward and easy to answer.",
        "This requires detailed medical history and is complex."
    ],  # Reference reasoning
    "reasoning_prediction": [
        "The questionnaire appears
 simple with no detailed medical requirements.",
        "It includes many questions about past medical history, suggesting complexity.",
        "Straightforward questions make this easy to complete.",
        "Requires extensive medical history, making it complex."
    ]  # Predicted reasoning
}

*italicized text*

df = pd.DataFrame(data)
```

```
Sample Output:
{'difficulty_metrics': {'accuracy': 0.75,
  'f1': 0.6666666666666666,
  'precision': 0.625,
  'recall': 0.75,
  'confusion_matrix': [[2, 0, 0], [0, 1, 0], [0, 1, 0]]},
 'reasoning_metrics': {'bert_score': {'precision': 0.9242371618747711,
   'recall': 0.9268922507762909,
   'f1': 0.9255314916372299},
  'meteor_score': 0.46955730750248714},
 'llm_judge_scores': {'mean_score': 2.75, 'scores': [3.0, 3.0, 2.0, 3.0]}}
 ```
"""

!pip install bert-score
!pip install nltk
import nltk
nltk.download('punkt_tab')
nltk.download('wordnet')

import requests
import re
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
from bert_score import score
from nltk.translate import meteor
from nltk.tokenize import word_tokenize

def get_openai_response(prompt, model):
    api_key = "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"
    org_key = "org-ewbrRzXdrHxv7hV0WyCFzGdD"

    """
    Sends a prompt to OpenAI's API and retrieves the response.

    Args:
        prompt (str): The prompt for the LLM.
        model (str): The model to use (e.g., 'gpt-4').
        api_key (str): OpenAI API key.
        org_key (str): OpenAI organization key.

    Returns:
        str: The response text from the LLM.
    """
    url = 'https://api.openai.com/v1/chat/completions'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}',
        'OpenAI-Organization': org_key
    }
    data = {
        'messages': [{'role': 'system', 'content': prompt}],
        'model': model,
        'temperature': 0.0
    }
    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        return response.json()['choices'][0]['message']['content']
    else:
        raise Exception(f"API request failed with status code {response.status_code}: {response.text}")

def llm_as_a_judge_prompt(conversation, reasoning_summary):
    """
    Generates a prompt for the LLM-as-a-judge evaluation.

    Args:
        conversation (str): The conversation or context text.
        reasoning_summary (str): The reasoning summary text.

    Returns:
        str: The generated prompt.
    """
    prompt = f"""### Instruction: Evaluate the reasoning for predicting the difficulty of medical questionnaires based on the conversation.

### Scoring Criteria:

**Case 1: Simple Questionnaire (Low Difficulty)**
- **2 points** if the reasoning clearly indicates that the questionnaire is simple with few questions, minimal medical history required, or if the conversation suggests an easy-to-understand questionnaire.
- **1 point** if the reasoning indicates that the questionnaire might be simple, but lacks clarity or supporting evidence from the conversation.
- **0 points** if no reasoning is provided or it contradicts the idea of simplicity.

**Case 2: Complex Questionnaire (High Difficulty)**
- **2 points** if the reasoning clearly indicates that the questionnaire is complex with multiple questions, detailed medical history required, or if the conversation suggests a high level of detail needed from the patient.
- **1 point** if the reasoning indicates that the questionnaire might be complex, but lacks enough supporting evidence from the conversation.
- **0 points** if no reasoning is provided or it contradicts the idea of complexity.

**General Evaluation Criteria:**
- **Clarity and Coherence**: 0.5 points for clear, well-structured reasoning.
- **Relevance**: 0.5 points if the reasoning is relevant to predicting the difficulty of the questionnaire based on the conversation.
- **Accuracy**: 1 point if the difficulty prediction aligns with the conversation content.

### Input:
- **Conversation**:
{conversation}

- **Summary (Reasoning for difficulty prediction)**:
{reasoning_summary}

### Output:
- "score: <total points>"
- Briefly justify your score, up to 50 words.
"""
    return prompt

def evaluate_difficulty_and_reasoning(df, model):
    """
    Evaluates the dataframe, including `LLM as a judge` metric.

    Args:
        df (pd.DataFrame): Input dataframe with columns for difficulty, predictions, and reasoning.
        model (str): The LLM model to use (e.g., 'gpt-4').
        api_key (str): OpenAI API key.
        org_key (str): OpenAI organization key.

    Returns:
        dict: A dictionary of metrics for difficulty, reasoning, and LLM as a judge.
    """
    # Validate required columns
    required_columns = ['difficulty', 'difficulty_prediction', 'reasoning', 'reasoning_prediction']
    if not all(col in df.columns for col in required_columns):
        raise ValueError(f"Dataframe must contain the following columns: {required_columns}")

    # Metrics for difficulty predictions
    difficulty_metrics = {
        "accuracy": accuracy_score(df['difficulty'], df['difficulty_prediction']),
        "f1": f1_score(df['difficulty'], df['difficulty_prediction'], average='weighted'),
        "precision": precision_score(df['difficulty'], df['difficulty_prediction'], average='weighted'),
        "recall": recall_score(df['difficulty'], df['difficulty_prediction'], average='weighted'),
        "confusion_matrix": confusion_matrix(df['difficulty'], df['difficulty_prediction']).tolist()
    }

    # Metrics for reasoning predictions
    reasoning_metrics = {
        "bert_score": {"precision": [], "recall": [], "f1": []},
        "meteor_score": []
    }
    llm_judge_scores = []
    for _, row in df.iterrows():
        ref = row['reasoning']
        pred = row['reasoning_prediction']
        # Compute BERTScore
        P, R, F1 = score([pred], [ref], lang='en', verbose=False)
        reasoning_metrics["bert_score"]["precision"].append(P.mean().item())
        reasoning_metrics["bert_score"]["recall"].append(R.mean().item())
        reasoning_metrics["bert_score"]["f1"].append(F1.mean().item())

        # Compute METEOR
        meteor_score_value = meteor([word_tokenize(ref)], word_tokenize(pred))
        reasoning_metrics["meteor_score"].append(meteor_score_value)

        # Generate LLM-as-a-judge prompt
        prompt = llm_as_a_judge_prompt(ref, pred)
        try:
            llm_response = get_openai_response(prompt, model)
            llm_score = extract_score_from_llm_response(llm_response)
        except Exception as e:
            print(f"Error in LLM scoring: {e}")
            llm_score = None
        llm_judge_scores.append(llm_score)

    # Aggregate BERTScore and METEOR
    reasoning_metrics["bert_score"] = {
        "precision": np.mean(reasoning_metrics["bert_score"]["precision"]),
        "recall": np.mean(reasoning_metrics["bert_score"]["recall"]),
        "f1": np.mean(reasoning_metrics["bert_score"]["f1"])
    }
    reasoning_metrics["meteor_score"] = np.mean(reasoning_metrics["meteor_score"])

    # Combine all metrics
    return {
        "difficulty_metrics": difficulty_metrics,
        "reasoning_metrics": reasoning_metrics,
        "llm_judge_scores": {
            "mean_score": np.nanmean(llm_judge_scores),
            "scores": llm_judge_scores
        }
    }

def extract_score_from_llm_response(response):
    """
    Extracts the score from LLM response text.

    Args:
        response (str): The text response from the LLM.

    Returns:
        float: The extracted score.
    """
    pattern = r"score:\s*(\d+(\.\d+)?)"
    match = re.search(pattern, response.lower())
    if match:
        return float(match.group(1))
    else:
        return None

data = {
    "difficulty": [1, 2, 1, 3],  # Ground truth difficulty levels
    "difficulty_prediction": [1, 2, 1, 2],  # Predicted difficulty levels
    "reasoning": [
        "The questionnaire is simple and does not require detailed medical history.",
        "The questionnaire involves multiple questions about medical history.",
        "The questions are straightforward and easy to answer.",
        "This requires detailed medical history and is complex."
    ],  # Reference reasoning
    "reasoning_prediction": [
        "The questionnaire appears simple with no detailed medical requirements.",
        "It includes many questions about past medical history, suggesting complexity.",
        "Straightforward questions make this easy to complete.",
        "Requires extensive medical history, making it complex."
    ]  # Predicted reasoning
}

df = pd.DataFrame(data)

result = evaluate_difficulty_and_reasoning(df, 'gpt-4o-mini')
print(result)