# -*- coding: utf-8 -*-
"""llm-fine-tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G8zshO3uBxpjjuB_fB99Wl9tX44nfLbY
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Installs Unsloth, Xformers (Flash Attention) and all other packages!
# !pip install unsloth
# # Get latest Unsloth
# !pip install --upgrade --no-deps "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

"""## Initializing the Model"""

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-v0.3-bnb-4bit",      # New Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit",           # Llama-3 15 trillion tokens model 2x faster!
    "unsloth/llama-3-8b-Instruct-bnb-4bit",
    "unsloth/llama-3-70b-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct",        # Phi-3 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",             # Gemma 2.2x faster!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)
from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-v0.3-bnb-4bit",      # New Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit",           # Llama-3 15 trillion tokens model 2x faster!
    "unsloth/llama-3-8b-Instruct-bnb-4bit",
    "unsloth/llama-3-70b-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct",        # Phi-3 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",             # Gemma 2.2x faster!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

"""# Dataset Preparation :
Loading the question_with_options along with the predicted_label and the gpt_reasoning information as a csv.
"""

from datasets import load_dataset
dataset = load_dataset(
    "csv",
    data_files = "dataset.csv",
    split = "train",
)
print(dataset.column_names)
print(dataset[0])

"""## Converting the dataset into required format:"""

from unsloth import to_sharegpt

# Define a post-processing function to combine 'predicted_label' and 'reasoning'
def combine_columns(example):
    example["predicted_label_and_reasoning"] = f"Label: {example['predicted_label']}, Reasoning: {example['gpt_reasoning']}"
    return example

# Apply the function to the dataset
dataset = dataset.map(combine_columns)

# Verify the formatted dataset
print(dataset[0])


# Merge "predicted_label" and "reasoning" into the response in the merged_prompt
dataset = to_sharegpt(
    dataset,
    merged_prompt="{questions_with_options}",
    output_column_name="predicted_label_and_reasoning",  # Combine predicted_label and reasoning
)


print(dataset[0])

"""## Standardize share-gpt"""

from unsloth import standardize_sharegpt
dataset = standardize_sharegpt(dataset)

"""## Chat Template"""

chat_template = """Below are questions with options. Provide the predicted label and reasoning.

>>> Question:
{INPUT}

>>> Answer:
{OUTPUT}

"""

from unsloth import apply_chat_template
dataset = apply_chat_template(
    dataset,
    tokenizer = tokenizer,
    chat_template = chat_template,
    # default_system_message = "You are a helpful assistant", << [OPTIONAL]
)

"""## Training the Model"""

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60, ## comment this later
        num_train_epochs = 5, ## train for 5 epochs ideally
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)

trainer_stats = trainer.train()

"""## Saving the fine-tuned model"""

# Step 9: Save the fine-tuned model
model.save_pretrained("fine_tuned_llama")
tokenizer.save_pretrained("fine_tuned_llama_tokenizer")

"""## Performing Inference using the excel file containing the ground truth label and reasoning

For this, we have taken the 100 records (50 easy, 50 difficult) containing the question with options, the ground truth label, as well as gpt-reasoning for that ground truth label.
"""

import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv("inference.csv")

# Display the first few rows of the DataFrame
print(df.head())

# Define the additional string that will be added to each content
additional_content = (
    "Please provide the output in the following format:\n\n"
    "Response:\n"
    "Difficulty Level: 0 or 1 (depending on difficulty)\n"
    "Reasoning: \"<Reasoning text>\"\n"
)



# Prepare the messages for inference by extracting the 'question with options' column
messages = [{"role": "user", "content": row["questions_with_options"] + additional_content} for _, row in df.iterrows()]

from transformers import TextStreamer
import re

FastLanguageModel.for_inference(model)  # Enable faster inference

responses = []  # To store model responses

# Define regex patterns to extract the label and reasoning
label_pattern = r"Label: (\d)"
reasoning_pattern = r"Reasoning: (.*)"

text_streamer = TextStreamer(tokenizer, skip_prompt=True)  # For streaming output

# Generate responses for each message
for idx, row in df.iterrows():
    # Prepare the message by adding additional content
    additional_content = (
        "Please provide the output in the following format:\n\n"
        "Response:\n"
        "Difficulty Level: 0 or 1 (depending on difficulty)\n"
        "Reasoning: \"<Reasoning text>\""
    )
    message = {"role": "user", "content": row["questions_with_options"] + additional_content}

    input_ids = tokenizer.apply_chat_template(
        [message],  # Each message as input
        add_generation_prompt=True,
        return_tensors="pt",
    ).to("cuda")

    generated_output = model.generate(
        input_ids,
        streamer=text_streamer,
        max_new_tokens=256,
        pad_token_id=tokenizer.eos_token_id
    )

    # Decode the generated output
    response_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)

    # Use regex to extract the predicted label and reasoning
    label_match = re.search(label_pattern, response_text)
    reasoning_matches = re.findall(reasoning_pattern, response_text)

    # Extract label and reasoning
    predicted_label = label_match.group(1) if label_match else None
    reasoning = reasoning_matches[1] if len(reasoning_matches) > 1 else None

    # Append the response and additional info to the responses list
    responses.append({
        "difficulty": row["ground_truth_label"],  # Ground truth difficulty
        "difficulty_prediction": predicted_label,  # Predicted difficulty
        "reasoning": row["gpt_reasoning"],    # Ground truth reasoning
        "reasoning_prediction": reasoning  # Predicted reasoning
    })

# Convert responses to a DataFrame
responses_df = pd.DataFrame(responses)

# Convert responses to a DataFrame
responses_df = pd.DataFrame(responses)
responses_df

"""## Evaluation Metrics and pre-defined function"""

!pip install bert-score
!pip install nltk
import nltk
nltk.download('punkt_tab')
nltk.download('wordnet')

import requests
import re
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
from bert_score import score
from nltk.translate import meteor
from nltk.tokenize import word_tokenize

def get_openai_response(prompt, model):
    api_key = "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"
    org_key = "org-ewbrRzXdrHxv7hV0WyCFzGdD"

    """
    Sends a prompt to OpenAI's API and retrieves the response.

    Args:
        prompt (str): The prompt for the LLM.
        model (str): The model to use (e.g., 'gpt-4').
        api_key (str): OpenAI API key.
        org_key (str): OpenAI organization key.

    Returns:
        str: The response text from the LLM.
    """
    url = 'https://api.openai.com/v1/chat/completions'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}',
        'OpenAI-Organization': org_key
    }
    data = {
        'messages': [{'role': 'system', 'content': prompt}],
        'model': model,
        'temperature': 0.0
    }
    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        return response.json()['choices'][0]['message']['content']
    else:
        raise Exception(f"API request failed with status code {response.status_code}: {response.text}")

def llm_as_a_judge_prompt(conversation, reasoning_summary):
    """
    Generates a prompt for the LLM-as-a-judge evaluation.

    Args:
        conversation (str): The conversation or context text.
        reasoning_summary (str): The reasoning summary text.

    Returns:
        str: The generated prompt.
    """
    prompt = f"""### Instruction: Evaluate the reasoning for predicting the difficulty of medical questionnaires based on the conversation.

### Scoring Criteria:

**Case 1: Simple Questionnaire (Low Difficulty)**
- **2 points** if the reasoning clearly indicates that the questionnaire is simple with few questions, minimal medical history required, or if the conversation suggests an easy-to-understand questionnaire.
- **1 point** if the reasoning indicates that the questionnaire might be simple, but lacks clarity or supporting evidence from the conversation.
- **0 points** if no reasoning is provided or it contradicts the idea of simplicity.

**Case 2: Complex Questionnaire (High Difficulty)**
- **2 points** if the reasoning clearly indicates that the questionnaire is complex with multiple questions, detailed medical history required, or if the conversation suggests a high level of detail needed from the patient.
- **1 point** if the reasoning indicates that the questionnaire might be complex, but lacks enough supporting evidence from the conversation.
- **0 points** if no reasoning is provided or it contradicts the idea of complexity.

**General Evaluation Criteria:**
- **Clarity and Coherence**: 0.5 points for clear, well-structured reasoning.
- **Relevance**: 0.5 points if the reasoning is relevant to predicting the difficulty of the questionnaire based on the conversation.
- **Accuracy**: 1 point if the difficulty prediction aligns with the conversation content.

### Input:
- **Conversation**:
{conversation}

- **Summary (Reasoning for difficulty prediction)**:
{reasoning_summary}

### Output:
- "score: <total points>"
- Briefly justify your score, up to 50 words.
"""
    return prompt

def evaluate_difficulty_and_reasoning(df, model):
    """
    Evaluates the dataframe, including `LLM as a judge` metric.

    Args:
        df (pd.DataFrame): Input dataframe with columns for difficulty, predictions, and reasoning.
        model (str): The LLM model to use (e.g., 'gpt-4').
        api_key (str): OpenAI API key.
        org_key (str): OpenAI organization key.

    Returns:
        dict: A dictionary of metrics for difficulty, reasoning, and LLM as a judge.
    """
    # Validate required columns
    required_columns = ['difficulty', 'difficulty_prediction', 'reasoning', 'reasoning_prediction']
    if not all(col in df.columns for col in required_columns):
        raise ValueError(f"Dataframe must contain the following columns: {required_columns}")

    # Metrics for difficulty predictions
    difficulty_metrics = {
        "accuracy": accuracy_score(df['difficulty'], df['difficulty_prediction']),
        "f1": f1_score(df['difficulty'], df['difficulty_prediction'], average='weighted'),
        "precision": precision_score(df['difficulty'], df['difficulty_prediction'], average='weighted'),
        "recall": recall_score(df['difficulty'], df['difficulty_prediction'], average='weighted'),
        "confusion_matrix": confusion_matrix(df['difficulty'], df['difficulty_prediction']).tolist()
    }

    # Metrics for reasoning predictions
    reasoning_metrics = {
        "bert_score": {"precision": [], "recall": [], "f1": []},
        "meteor_score": []
    }
    llm_judge_scores = []
    for _, row in df.iterrows():
        ref = row['reasoning']
        pred = row['reasoning_prediction']
        # Compute BERTScore
        P, R, F1 = score([pred], [ref], lang='en', verbose=False)
        reasoning_metrics["bert_score"]["precision"].append(P.mean().item())
        reasoning_metrics["bert_score"]["recall"].append(R.mean().item())
        reasoning_metrics["bert_score"]["f1"].append(F1.mean().item())

        # Compute METEOR
        meteor_score_value = meteor([word_tokenize(ref)], word_tokenize(pred))
        reasoning_metrics["meteor_score"].append(meteor_score_value)

        # Generate LLM-as-a-judge prompt
        prompt = llm_as_a_judge_prompt(ref, pred)
        try:
            llm_response = get_openai_response(prompt, model)
            llm_score = extract_score_from_llm_response(llm_response)
        except Exception as e:
            print(f"Error in LLM scoring: {e}")
            llm_score = None
        llm_judge_scores.append(llm_score)

    # Aggregate BERTScore and METEOR
    reasoning_metrics["bert_score"] = {
        "precision": np.mean(reasoning_metrics["bert_score"]["precision"]),
        "recall": np.mean(reasoning_metrics["bert_score"]["recall"]),
        "f1": np.mean(reasoning_metrics["bert_score"]["f1"])
    }
    reasoning_metrics["meteor_score"] = np.mean(reasoning_metrics["meteor_score"])

    # Combine all metrics
    return {
        "difficulty_metrics": difficulty_metrics,
        "reasoning_metrics": reasoning_metrics,
        "llm_judge_scores": {
            "mean_score": np.nanmean(llm_judge_scores),
            "scores": llm_judge_scores
        }
    }

def extract_score_from_llm_response(response):
    """
    Extracts the score from LLM response text.

    Args:
        response (str): The text response from the LLM.

    Returns:
        float: The extracted score.
    """
    pattern = r"score:\s*(\d+(\.\d+)?)"
    match = re.search(pattern, response.lower())
    if match:
        return float(match.group(1))
    else:
        return None

responses_df

# Drop rows where 'difficulty_prediction' is None or NaN
responses_df = responses_df.dropna(subset=['difficulty_prediction'])

# Convert the 'difficulty' and 'difficulty_prediction' columns to integers
responses_df['difficulty'] = responses_df['difficulty'].astype(int)
responses_df['difficulty_prediction'] = responses_df['difficulty_prediction'].astype(int)

# Now, you can run your evaluation function
result = evaluate_difficulty_and_reasoning(responses_df, 'gpt-4o-mini')

print(result)